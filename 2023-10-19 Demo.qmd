---
title: "MD06 Demo, Part I"
format: html
editor: visual
---

Load packages and set theme.

```{r}
#| message: false
library(tidyverse)
library(moderndive)
library(ggthemes)
library(patchwork)
theme_set(theme_light())
```

Let's look at the MD evaluations data.

```{r}
data(evals)
glimpse(evals)
```

We are going to ask whether the "beauty" of an instructor predicts their teaching evaluations.

```{r}
d <- evals |>
  rename(bty = bty_avg,    # just shorter to type
         sex = gender)     # actually what they have

glimpse(d)
```

Let's look at the first few cases.

```{r}
head(d)
```

Here's the regression from last week.

```{r}
mod1 <- lm(score ~ bty,
           data = d)

get_regression_table(mod1)
```

*Summary(d\$bty) in console will tell you the min, 1st quarter, max, etc*

*Max(d\$bty)\*0.067-min(d\$bty)\*0.067 -\> the biggest difference beauty could have on evaluation score is 0.4335*

Let's look at the predictions and residuals.

```{r}
mod1_preds <- get_regression_points(mod1)

head(mod1_preds)
```

Here's the regression line (blue).

```{r,echo=FALSE}
ggplot(d,
       aes(x = bty,
           y = score)) +
  geom_jitter(alpha = .3) +   #scatter plot
  geom_smooth(method = "lm",
              se = FALSE) +   #line through the scatter plot
  labs(x = "Beauty",
       y = "Evaluation",
       title = "Simple regression results")
```

*Intercept = 3.88 & Slope = 0.067 --\> when someone has a beauty score of 0 we expect them to get an evaluation of 3.88 (has no statistical meaning because no one has a beauty score of 0) --\> increase in beauty score by 1 is related to a 0.067 increase in evaluation score*

Here are the residuals.

*is the answer higher or lower than what we would expect?*

```{r,echo=FALSE}
ggplot(mod1_preds,
       aes(x = bty,
           y = residual)) +
  geom_jitter(alpha = .3) +
  geom_hline(yintercept = 0,
             color = "blue") +
  labs(x = "Beauty",
       y = "Residual",
       title = "Simple regression residuals")
```

One way to quantify how well a predictor predicts the outcome is to use the **variance**. We've seen this already but let's review. Here's the formula. $\bar{y}$ is the mean of $y$.

$$
V(y) = \frac{1}{n-1}\sum_{i=1}^{n}(y_i - \bar{y})^2
$$

Since our outcome is evaluation score, we can just do that.

*Only using the scores here, used to see how much beauty helps us to make a predict, we need to start by seeing how much variance there is in the first place ("some do, some don't").*

*Regression is the study of variance and how we interpret it*

```{r}
var_y <- d |> 
  pull(score) |> 
  var()

var_y
```

This is equivalent to the variance of the "residuals" when we just guess the mean value for every person!

```{r, echo=FALSE}
ggplot(d,
       aes(x = bty,
           y = score)) +
  geom_jitter(alpha = .3) +
  geom_hline(yintercept = mean(d$score),
             color = "blue") +
  labs(x = "Beauty",
       y = "Evaluation",
       title = "Guessing the mean for everyone")
```

*Variance is a regression where we assume the slope is 0. Regardless of their beauty, I'm gonna guess the same thing for everyone (Average).*

Adding `beauty` as a predictor improves our guess.

```{r, echo=FALSE}
ggplot(d,
       aes(x = bty,
           y = score)) +
  geom_jitter(alpha = .3) +
  geom_hline(yintercept = mean(d$score),
             color = "blue") +
  geom_smooth(method = "lm",
              se = FALSE,
              color = "red",
              linetype = "dashed") +
  labs(x = "Beauty",
       y = "Evaluation",
       title = "Mean vs. regression line")
```

*Will the red line give you a better answer than the blue line? YES. Using additional information will always improve your guess within the sample. In a way this is cheating, using your sample to make predictions about the sample*

Now let's see what the spread looks like if we look at the residuals from the regression line.

```{r}
var_yhat1 <- mod1_preds |> 
  pull(residual) |> 
  var()

var_yhat1
```

*Variance = (Observation - Mean) \^2*

*Variance decreased slightly from 0.29*

If we take the ratio of these and subtract it from one, that gives us the $R^2$ or "proportion of variance explained" by beauty scores.

```{r}
1 - (var_yhat1 / var_y)
```

It looks like "beauty" can account for about `r round((1 - (var_yhat1 / var_y)) * 100, 1)` percent of the variance in the evaluation scores.

In other words, if we try to guess instructors' evaluation scores, our guesses will be `r round((1 - (var_yhat1 / var_y)) * 100, 1)` percent *less wrong* on average if we know the instructor's beauty rating.

We can get this from `broom::glance()` or `moderndive::get_regression_summaries()` without doing it manually. The latter will be a more curated list of things you'll need for this course.

```{r}
broom::glance(mod1)
moderndive::get_regression_summaries(mod1)
```

Let's try a different predictor. Let's predict ratings with `sex`.

```{r}
mod2 <- lm(score ~ sex,
           data = d)

get_regression_table(mod2)
get_regression_summaries(mod2)
```

Looks like male instructors get slightly better evaluations but this only accounts for a tiny bit of the of the variance. Don't be confused however: small amounts of variance can be really important even if they don't tell the whole story of variability.

In general, we are not going to want to do this with different variables one by one. We want to do **multiple regression**.

```{r}
mod3 <- lm(score ~ bty + sex,
           data = d)

get_regression_table(mod3)
```

Here's what this model does.

```{r, echo=FALSE}
ggplot(d,
       aes(x = bty,
           y = score,
           group = sex,
           color = sex)) +
  geom_jitter(alpha = .3) +
  geom_parallel_slopes(se = FALSE) +  # this is a modern dive thing!
  theme(legend.position = "top")
```

Here's the formula:

$$
\widehat{score}_i = 3.75 + .074(beauty_i) + .172(male_i)
$$

We might instead prefer a model like this, which allows the relationship between beauty and evaluations to be *different* for male and female instructors.

```{r, echo=FALSE}
ggplot(d,
       aes(x = bty,
           y = score,
           group = sex,
           color = sex)) +
  geom_jitter(alpha = .3) +
  geom_smooth(method = "lm",
              se = FALSE) +
  theme(legend.position = "top")
```

Here's the syntax, results, and formula.

```{r}
mod4 <- lm(score ~ bty + sex + bty:sex,
           data = d)

get_regression_table(mod4)
```

$$
\widehat{score}_i = 3.95 + .031(beauty_i) + -.184(male_i) + .080(beauty_i \times male_i)
$$ The slope for female instructors is .031. The slope for male instructors is .031 + .080 = .111.

```{r}
get_regression_summaries(mod3) # parallel
get_regression_summaries(mod4) # interactions
```

Do you think this is an important improvement?
